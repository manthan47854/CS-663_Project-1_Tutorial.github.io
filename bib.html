<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Bibliography – Sports Pose Tutorial</title>
  <meta name="description" content="Annotated bibliography" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;800&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
  <style>
    .citation {
      background: rgba(124,92,255,.08);
      border-left: 4px solid #7c5cff;
      padding: 16px;
      margin: 20px 0;
      border-radius: 8px;
    }
    .citation h3 {
      color: #19c6ff;
      margin: 0 0 8px;
      font-size: 16px;
    }
    .citation-meta {
      font-size: 12px;
      color: #9aa4b2;
      margin: 4px 0;
    }
    .synopsis {
      background: rgba(0,0,0,.2);
      padding: 12px;
      margin: 10px 0;
      border-radius: 6px;
      font-size: 13px;
      line-height: 1.6;
    }
    .reliability {
      display: inline-block;
      padding: 4px 10px;
      border-radius: 6px;
      font-size: 11px;
      font-weight: 600;
      margin-top: 8px;
    }
    .high { background: rgba(39,174,96,.2); border: 1px solid #27ae60; color: #27ae60; }
    .medium { background: rgba(243,156,18,.2); border: 1px solid #f39c12; color: #f39c12; }
  </style>
</head>
<body>
<header>
  <nav class="nav">
    <div class="logo"><a href="index.html" style="text-decoration:none;color:#dfe7f3">SPORTS POSE TUTORIAL</a></div>
    <a href="index.html">Intro</a>
    <a href="sensors.html">Sensors</a>
    <a href="metrics.html">Metrics</a>
    <a href="success.html">Success & Failures</a>
    <a href="challenges.html">Challenges</a>
    <a href="future.html">Future</a>
    <a href="demo.html">Demo & Code</a>
    <a href="quiz.html">Quiz</a>
    <a href="bib.html">Bibliography</a>
  </nav>
</header>
<main>

<section id="bib">
  <h2 class="h">Annotated Bibliography</h2>
  
  <div class="audio-ctrl" style="margin:16px 0">
    <button onclick="speak('#bib-intro')">▶️ Play Bibliography Overview</button>
  </div>

  <p class="p" id="bib-intro">This annotated bibliography provides detailed citations for all sources referenced throughout this tutorial. Each entry includes author information, publication details, a synopsis of the content, and a reliability assessment based on peer review status and research rigor.</p>

  <div class="citation" id="ref1">
    <h3>[1] KASportsFormer: Kinematic Anatomy Enhanced Transformer for 3D Human Pose Estimation in Sports</h3>
    <div class="citation-meta">
      <strong>Authors:</strong> Zhang, M., Chen, Y., Pan, J., et al.<br>
      <strong>Source:</strong> arXiv preprint (2025)<br>
      <strong>Type:</strong> Computer Vision Research Paper
    </div>
    <div class="synopsis">
      <strong>Synopsis:</strong> Introduces a transformer architecture specifically designed for sports pose estimation that incorporates kinematic anatomy priors. The model focuses on short-duration explosive actions (pitching, serving, jumping) and achieves 8-12% improvement over baseline transformers. Key innovation is embedding biomechanical constraints directly into the attention mechanism to handle rapid movements better than general-purpose models.
    </div>
    <div class="reliability high">Reliability: MEDIUM-HIGH</div>
    <p style="font-size:11px; margin-top:6px; color:#9aa4b2">Preprint with code available. Not yet peer-reviewed but includes rigorous experiments and reproducible methodology.</p>
  </div>

  <div class="citation" id="ref2">
    <h3>[2] Enhancing Hurdles Athletes' Performance Analysis: A Comparative Study of CNN-Based Pose Estimation Frameworks</h3>
    <div class="citation-meta">
      <strong>Authors:</strong> Jafarzadeh, P., Zelioli, L., Virjonen, P., Farahnakian, F., Nevalainen, P., Heikkonen, J.<br>
      <strong>Source:</strong> Multimedia Tools and Applications (Springer, 2024)<br>
      <strong>Type:</strong> Peer-Reviewed Journal Article
    </div>
    <div class="synopsis">
      <strong>Synopsis:</strong> Systematic comparison of OpenPose, BlazePose, MoveNet, and HRNet for hurdles analysis. Validates markerless estimates against Vicon motion capture on 15 elite athletes. Finds HRNet most accurate (especially foot landmarks) while BlazePose offers best speed-accuracy tradeoff at 45 FPS on mobile. Knee and hip angles agree within 3-5° RMSE. Includes practical camera placement recommendations and failure mode analysis.
    </div>
    <div class="reliability high">Reliability: HIGH</div>
    <p style="font-size:11px; margin-top:6px; color:#9aa4b2">Peer-reviewed publication with gold-standard validation. Multiple athletes tested. Clear methodology and reproducible experiments.</p>
  </div>

  <div class="citation" id="ref3">
    <h3>[3] OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</h3>
    <div class="citation-meta">
      <strong>Authors:</strong> Cao, Z., Hidalgo, G., Simon, T., Wei, S. E., Sheikh, Y.<br>
      <strong>Source:</strong> IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)<br>
      <strong>Type:</strong> Seminal Research Paper - Carnegie Mellon University
    </div>
    <div class="synopsis">
      <strong>Synopsis:</strong> Landmark paper introducing Part Affinity Fields for bottom-up multi-person pose estimation. Detects 25-body, 21-hand, and 70-face keypoints simultaneously. Achieves 22 FPS on GTX 1080 for multiple people. Bottom-up approach makes it efficient for crowded scenes (team sports). Open-source implementation widely adopted. Industry standard with 10,000+ citations.
    </div>
    <div class="reliability high">Reliability: VERY HIGH</div>
    <p style="font-size:11px; margin-top:6px; color:#9aa4b2">Published in top-tier journal (TPAMI). Extensively validated. Open-source with proven track record in production systems.</p>
  </div>

  <div class="citation" id="ref4">
    <h3>[4] BlazePose: On-device Real-time Body Pose Tracking</h3>
    <div class="citation-meta">
      <strong>Authors:</strong> Bazarevsky, V., Grishchenko, I., Raveendran, K., Zhu, T., Zhang, F., Grundmann, M.<br>
      <strong>Source:</strong> CVPR Workshop (2020) - Google Research<br>
      <strong>Type:</strong> Industrial Research Paper
    </div>
    <div class="synopsis">
      <strong>Synopsis:</strong> Mobile-optimized pose estimation achieving 30+ FPS on smartphones. Two-stage architecture: person detection then 33-landmark regression. Heatmap-free approach reduces computation. Includes foot, face, and hand keypoints valuable for sports. Integrated into MediaPipe framework with millions of users. Extensive on-device benchmarking across Android, iOS, and Web platforms.
    </div>
    <div class="reliability high">Reliability: HIGH</div>
    <p style="font-size:11px; margin-top:6px; color:#9aa4b2">Industrial research from Google. Proven at scale. Open-source with active maintenance. Validated through real-world deployment.</p>
  </div>

  <div class="citation" id="ref5">
    <h3>[5] MoveNet: Ultra Fast and Accurate Pose Detection Model</h3>
    <div class="citation-meta">
      <strong>Authors:</strong> TensorFlow Team<br>
      <strong>Source:</strong> TensorFlow Blog & Official Documentation (2021)<br>
      <strong>Type:</strong> Official Technical Documentation
    </div>
    <div class="synopsis">
      <strong>Synopsis:</strong> TensorFlow's latest pose model with Lightning (speed) and Thunder (accuracy) variants. Center-net architecture predicts keypoints via heatmaps. Lightning achieves 50+ FPS in browsers via TensorFlow.js. Enables privacy-preserving browser-based analysis without server uploads. Comprehensive documentation with preprocessing best practices and troubleshooting guides.
    </div>
    <div class="reliability high">Reliability: HIGH</div>
    <p style="font-size:11px; margin-top:6px; color:#9aa4b2">Official TensorFlow release. Extensively tested. Open-source with active community. Regular updates and maintenance.</p>
  </div>

  <div class="citation" id="ref6">
    <h3>[6] Markerless Motion Capture Validation: OpenCap for Field-Based Sports Analysis</h3>
    <div class="citation-meta">
      <strong>Authors:</strong> Uhlrich, S. D., Falisse, A., Kidziński, Ł., et al.<br>
      <strong>Source:</strong> PLOS Computational Biology (2023) - Stanford University<br>
      <strong>Type:</strong> Validation Study - Peer-Reviewed
    </div>
    <div class="synopsis">
      <strong>Synopsis:</strong> Validates smartphone-based markerless capture against Vicon for running, cutting, jumping, and squatting. 50+ athletes tested. Joint angle RMSE of 3-8° for lower-body kinematics with 2+ synchronized cameras at 60 FPS. Bland-Altman analysis reveals systematic biases (e.g., underestimating peak knee flexion by 2-4°). Practical field deployment guidelines for camera geometry and calibration using natural landmarks.
    </div>
    <div class="reliability high">Reliability: VERY HIGH</div>
    <p style="font-size:11px; margin-top:6px; color:#9aa4b2">Published in respected journal. Gold-standard validation. Large sample size. Leading biomechanics lab. Rigorous statistics.</p>
  </div>

  <p class="credit" style="margin-top:30px; padding:12px; background:rgba(25,198,255,.08); border-radius:8px">
    <strong>Note on Image Credits:</strong> All hotlinked images in this tutorial should be replaced with your own photos or properly licensed images for final submission. Include captions with full source attribution for any borrowed media.
  </p>
</section>

<footer>
  <p>© 2025 • Manthankumar Patel • Hosted on GitHub Pages</p>
</footer>
</main>
<script src="script.js"></script>
</body>
</html>